{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nolauren/colabs/blob/main/DVT_NUS_Workbook_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu4yjY47mBkc"
      },
      "source": [
        "**Distant Viewing Toolkit**\n",
        "\n",
        "For more information about the toolkit and the Distant Viewing Lab, please see our [GitHub page](https://github.com/distant-viewing/dvt)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g4zyGhYaHIYS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doDqBIYf_bBb"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "- How can we look at images through computers?\n",
        "- How might they help us \"see\" and \"view\" in expected and unexpected ways?\n",
        "\n",
        "\n",
        "This notebook uses the Distant Viewing Toolkit (DVT) to analyze color photographs from the 1930s and 1940s. Using computer vision, DVT facilitates the computational analysis of (moving) images. Specifically designed for the study of visual culture, the toolkit enacts the DV method. See our article in DSH at https://doi.org/10.1093/llc/fqz013 for more.\n",
        "\n",
        "\n",
        " A note about this notebook: This document displays a file known as a Jupyter notebook. In this case, it contains a mix of plain text (like this one!) and code in the open-source programing language Python. The notebook is being hosted on Google's Colab platform, which allows us to run the code for free on a third-party system without the need to install Python and its many dependencies on our local machine. If you are interested in using these methods further, however, it is possible to install all of this on your machine and run the code locally. See the [INSTALL.md](https://github.com/distant-viewing/dvt/blob/main/INSTALL.md) file for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3magyHjmyth"
      },
      "source": [
        "# 2. Setup\n",
        "\n",
        "### 2.1 Installation\n",
        "\n",
        "The Google Colab environment already has a running version of Python and several of the most common modules (third-party code that extends the basic language). We only need to install the Distant Viewing Toolkit, which will also load a few extra dependencies. To do this, hover over the code block below and click on the run button that appears in the upper left corner of the block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrSRn5T9_Uds"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/distant-viewing/dvt.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWAr_-_qnKxU"
      },
      "source": [
        "It is possible that you may see one or two errors about extra dependencies. Our experience is that these can be ignored for the moment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMXDq-mJ_qST"
      },
      "source": [
        "### 2.2 Load Data\n",
        "\n",
        "Now that we have the Python modules installed, we next need to  download our image data. This can be done by running the following code block. We'll also create a directory to put all of the deep learning models that get downloaded in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cpHc2Cm_p4o"
      },
      "outputs": [],
      "source": [
        "!wget -q https://distantviewing.org/fsa_color.zip\n",
        "!unzip -q fsa_color.zip\n",
        "!mkdir -p /root/.cache/torch/hub/checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWmpTVqYnXEQ"
      },
      "source": [
        "Once finished, you will have all of the images downloaded to the Colab. Let's load all of the metadata for the collection."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A note about the data: We often use .jpeg files for our work. There is usually a file with the images and then a .csv file with the file name and relevant metadata. For example, you can download the fsa_color.zip directly to your machine and see how we set it up. There is a folder with the images as jpegs. We reduced the size for the purpose of this tutorial in Colab. In general, we find images around 2-3MB to be best for photographs since it captures significant visual data without being unnecessarily large. There is also a .csv file with the path name to each file and metadata that further describes contextual information for each image such as photographer and year taken."
      ],
      "metadata": {
        "id": "jJb0G5pKE0L1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPMgfdhXFEeV"
      },
      "source": [
        "### 2.3 Load Modules\n",
        "\n",
        "As a final setup task, we need to load in all of the modules that we will use. Just run the following block to load these functions and classes. We will explain what each of these does during the demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nrC5NvEFHYk"
      },
      "outputs": [],
      "source": [
        "import dvt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def show_img(img):\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    cv2_imshow(img_rgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Gtb8xSnuGh"
      },
      "source": [
        "Note that you will ideally not see any output in the previous block, which means that everything loaded without a problem. If you did have an error, this is likely an indication that a more serious problem has occured.\n",
        "\n",
        "\n",
        "\n",
        "*   dvt = [Distant Viewing Toolkit ](https://github.com/distant-viewing/dvt) is our library for supporting computer vision.\n",
        "*  numpy = [NumPy](https://numpy.org/) is for working with arrays and matrices.\n",
        "\n",
        "*   pandas = [pandas](https://pandas.pydata.org/) is for data analysis and manipulation.\n",
        "*   cv2 = [OpenCV](https://opencv.org/) is a computer vision library.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xs1CKZYSGg6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Metadata\n",
        "\n",
        "Before we start diving into the images, let's load the image metadata and take a moment to understand what information exists for these photographs."
      ],
      "metadata": {
        "id": "rj1R6YAMll3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3dOLvhmsuFy"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "\n",
        "dt = pandas.read_csv(\"fsa_color/fsa_color_metadata_subset.csv\")\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Digital Images\n",
        "\n",
        "Briefly, let's start by understanding how digitial images are stored inside of Python. We'll load into Python one of the images that we downloaded and then look at the structure. Reading in the image with the function `dvt.load_image` and using our wrapper function `show_img` displays the image in much the way you would see it on a typical website."
      ],
      "metadata": {
        "id": "Ih-hW6Dkl0Ef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjfxximuFzPz"
      },
      "outputs": [],
      "source": [
        "img = dvt.load_image(\"fsa_color/images/\" + dt.path[94])\n",
        "show_img(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual image data in Python, though, is stored as grids of numbers. Specifically, we have the rectangular grids of pixel intensities corresponding to the colors red, green, and blue. We can see the shape of these data using the `shape` method:"
      ],
      "metadata": {
        "id": "hQOaQot7mrOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "id": "HG4X4yBTmqu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try to look at small slices of the pixel intensities by using the following notation in Python:"
      ],
      "metadata": {
        "id": "pFOZL7oRnRwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img[50:55, 600:605, 0]"
      ],
      "metadata": {
        "id": "oirrIvmynRgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The challenge is that we won't get very far trying to interpret the photographs by looking at these individual numbers."
      ],
      "metadata": {
        "id": "zn_chs7xnq6S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oetxTx0Fyt9"
      },
      "source": [
        "# 5. Annotating One Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrEbXR7qozcg"
      },
      "source": [
        "### 5.1 Initial Annotators\n",
        "\n",
        "While many annotation that we might use to provide structured information describing an image requires complex deep learning models, this is not a requirement. We can also apply relatively simple algorithms to determine things such as the average value (brightness) or saturation (richness) of the image. The brightness comes from just the average of the pixel intensities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XQTee5lGI_2"
      },
      "outputs": [],
      "source": [
        "np.mean(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7agU8JVZpKCI"
      },
      "source": [
        "Saturation requires using a different color space, to which we can then apply the average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8XU33oRGdtD"
      },
      "outputs": [],
      "source": [
        "img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "np.mean(img_hsv[:, :, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMGhF3Gtpgn3"
      },
      "source": [
        "This may not be very interesting on its own, but both of these measurements can reveal interesting patterns when aggregated across a larger collection of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGgqkE2rHxs6"
      },
      "source": [
        "### 5.2 Detecting and Identifying Faces\n",
        "\n",
        "Now that we understand the basic structure of the toolkit, let's work through some more complex annotators. For example, we will use the `AnnoFaces` to detect all of the faces in our example image. We pass both a detector (to find the faces) as well as an embedding (to identify the faces)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p_OkGM5Kr1Z"
      },
      "outputs": [],
      "source": [
        "anno_face = dvt.AnnoFaces()\n",
        "out_face = anno_face.run(img, visualize=True)\n",
        "pd.DataFrame(out_face['boxes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlNSgz3iuD-q"
      },
      "source": [
        "The annotation includes detections of the image, which gives a bounding box for each face and a confidence score. The final column provides an *image embedding*. If we applied this to a larger collection, we could identify people across images/frames by associating embeddings that are very close to one another."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_img(out_face['img'])"
      ],
      "metadata": {
        "id": "EdC05v2UdEO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IRcGin6Ksbv"
      },
      "source": [
        "### 5.3 Image Embedding\n",
        "\n",
        "We can also apply an embedding to the entire image. These are useful for doing visual search, building recommendor systems, and doing visualizations of a larger corpus. Here is an example embedding a single image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cwd_W1wVKu2U"
      },
      "outputs": [],
      "source": [
        "anno_embed = dvt.AnnoEmbed()\n",
        "out_embed = anno_embed.run(img)\n",
        "out_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNpR2MlkvHv3"
      },
      "source": [
        "The output gives an *embedding* as a sequence of 1280 numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PizK22-ZMsaF"
      },
      "outputs": [],
      "source": [
        "out_embed['embedding'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "litnqqWdKvFa"
      },
      "source": [
        "### 5.4 Instance Segmentation\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "Another annotator that exists in the toolkit conducts instance annotation. This annotators try to locate common objects and people in the frame of the image. Unlike the face detector, it tries to find entire people rather than only detecting faces. Here is what the structured output look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjrqo1PIK8eW"
      },
      "outputs": [],
      "source": [
        "anno_detect = dvt.AnnoDetect()\n",
        "pd.DataFrame(anno_detect.run(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCBY8kDt03Fk"
      },
      "source": [
        "# 6. Applying to the Whole Collection\n",
        "\n",
        "### 6.1 Compute Simple Annotator on All Images\n",
        "\n",
        "Looking at one image is interesting, but what if we apply an annotator to the entire collection? We can do that with the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RIh7D9m03a8"
      },
      "outputs": [],
      "source": [
        "output = {'path': [], 'value': [], 'saturation': []}\n",
        "for fname in dt.path.values:\n",
        "    img = dvt.load_image(\"fsa_color/images/\" + fname)\n",
        "    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "    avg_saturation = np.mean(img_hsv[:, :, 1])\n",
        "    output['path'] += [fname]\n",
        "    output['value'] += [np.mean(img_hsv[:, :, 2])]\n",
        "    output['saturation'] += [np.mean(img_hsv[:, :, 1])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXs98EWV11s1"
      },
      "source": [
        "Here is what the averages data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrXKzIQ41noy"
      },
      "outputs": [],
      "source": [
        "output = pd.DataFrame(output)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can sort this data to find the brightest images."
      ],
      "metadata": {
        "id": "1bu0IuPZgn3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.sort_values(by=['value'], ascending=False)"
      ],
      "metadata": {
        "id": "Aai8NaN6hk5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then, we can look at the images with the most extreme values:"
      ],
      "metadata": {
        "id": "wru-DgJlg9IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = dvt.load_image(\"fsa_color/images/\" + \"1a34792v.jpg\")\n",
        "show_img(img)"
      ],
      "metadata": {
        "id": "48GUrf65goPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEDRD0ec2bhy"
      },
      "source": [
        "### 6.2 Nearest Embedding\n",
        "\n",
        "We can also apply the embedding annotator, which will enable finding nearest neighbors. Note that it will take a few minutes to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi0UUFzj2g3Q"
      },
      "outputs": [],
      "source": [
        "anno_embed = dvt.AnnoEmbed()\n",
        "out_embed = anno_embed.run(img)\n",
        "\n",
        "X = np.zeros((dt.shape[0], 1280))\n",
        "for ival, fname in enumerate(dt.path.values):\n",
        "    img = dvt.load_image(\"fsa_color/images/\" + fname)\n",
        "    out_embed = anno_embed.run(img)\n",
        "    X[ival, :] = out_embed['embedding']\n",
        "\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_tp5Mv73DSI"
      },
      "source": [
        "Now, we can compute the images that are the closest to any starting image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi_oFZQr4uCY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (16, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkF2cYfC3D83"
      },
      "outputs": [],
      "source": [
        "ref_img_num = 400       # change this number!\n",
        "\n",
        "idx = np.argsort(np.sum(np.abs(X - X[ref_img_num, :])**2, axis=1))[:12]\n",
        "for ind, i in enumerate(idx):\n",
        "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "    plt.subplot(4, 3, ind + 1)\n",
        "\n",
        "    img = dvt.load_image(\"fsa_color/images/\" + dt.path[i])\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYnyke_E4oF"
      },
      "source": [
        "And we can also get metadata information for the starting image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WdptOAeD5DL"
      },
      "outputs": [],
      "source": [
        "dt.iloc[[ref_img_num]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yl_90WvoaNu"
      },
      "source": [
        "## **7**. Conclusions and Cautions\n",
        "\n",
        "Applying computer vision algorithms to large collections of culturally important images and moving images offers some exciting possibilities. The\n",
        "Distant Viewing Toolkit was designed to lower the barrier of doing this and\n",
        "make the possibilities more accessible to a wide range of interested users.\n",
        "However, there remain many technical and ethical challenges for the application\n",
        "of this work that should not be ignored. We encourage you to explore the\n",
        "toolkit, while being aware of the potential issues and unintended consequences\n",
        "that these methods could exacerbate.\n",
        "\n",
        "To pursue a much deeper dive, *Distant Viewing: Computational Analysis of Digital Images* is coming out this fall with MIT Press. Open access! For more info, visit: https://mitpress.mit.edu/9780262546133/distant-viewing/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If you are interested in discuss these issues and areas of application, please\n",
        "reach out the Distant Viewing Lab's directors Taylor Arnold\n",
        "(tarnold2@richmond.edu) and Lauren Tilton (ltilton@richmond.edu).\n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}